Серебряков А.В.
Основные понятия теории нейронных сетей

\textbf{Нейронная сеть} — это вычислительная модель, вдохновленная строением мозга. Она состоит из взаимосвязанных и взвешенных "нейронов", организованных в слои. Нейроны передают сигналы через синапсы, а веса определяют влияние связей. Процесс обучения подразумевает коррекцию весов на основе ошибок предсказаний. Нейронные сети широко применяются в обработке изображений, распознавании речи, анализе текста и других областях, позволяя компьютерам обучаться и выполнять сложные задачи.
\textbf{Нейрон в нейронной сети} — базовая вычислительная единица, моделирующая нейроны в мозге. Он принимает входные сигналы, взвешивает их, применяет функцию активации, и передает выход другим нейронам. Веса связей между нейронами подстраиваются в процессе обучения, что позволяет сети адаптироваться к задачам, таким как распознавание образов или прогнозирование.
Основными функциональными элементами традиционно считаются: синапс, адаптивный сумматор, активатор, точка ветвления. Из этих простых элементов строятся более сложные – формальные нейроны. \\Линейная связь (синапс) — элемент, умножающий входной сигнал $x$ на "вес синапса" $\alpha$. \\Адаптивный сумматор – элемент, вычисляющий скалярное произведение входного сигнала $x$ на вектор параметров $\overline{\alpha\}$. Адаптивным он называется из-за наличия вектора настраиваемых параметров $\overline{\alpha\}$. \\Нелинейный преобразователь сигнала (функция активации нейрона, активатор) – элемент, получающий скалярный входной сигнал $x$ и переводящий его в $\phi(x)$. \\Точка ветвления - получает скалярный входной сигнал $x$  и передает его всем своим выходам. Такие выходы называют аксонами. \\Стандартный формальный нейрон – элемент, составленный из входного сумматора, нелинейного преобразователя и точки ветвления на выходе.
Под \textbf{формальным нейроном} понимается объект вида $F = (X, Y, S, g)$, где $S = {\{\overline{s}\}} \subseteq R^{N_s}$ - множество состояний, $X = {\{\overline{x}\}}\subseteq \{R\bigcup e\}^{N_x}$ - множество выходных сигналов, $e\notin R$ - элемент, используемый для обозначения отсутствия сигнала, $g:S\times X\to Y$ - функция преобразования $\overline{y}:= g(\overline{s}, \overline{x})$. Как правило, функция $g$ определяется формулами $y=\varphi(z)$ и $z=\sum_{i=1}^{N_x}x_is_i$, где $N_x=N_s$ - число входов нейрона; $s_i$ - вес синапса $(i=1, … ,n)$; $x_i$ - компонента вектора входного сигнала $(i=1, … ,n)$; $z$ - результат суммирования; $\varphi(z)$ - нелинейное преобразование; $y$ - выходной сигнал нейрона.
\textbf{Вектор весов нейрона} — это набор числовых параметров, используемых для умножения входных сигналов в нейроне. Каждый входной сигнал умножается на соответствующий вес, и результаты суммируются для получения взвешенной суммы. Формально, для нейрона с $n$ входами, вектор весов обозначается как $\mathbf{w} = [w_1, w_2, \ldots, w_n]$, где каждый $w_i$ — это вес для соответствующего входа $x_i$. Вместе с весами в уравнение вводится смещение (bias) $b$, и взвешенная сумма может быть записана как: $ z = \sum_{i=1}^{n} (w_i \cdot x_i) + b $. Этот вектор весов и смещение подстраиваются в процессе обучения нейронной сети с целью минимизации ошибки на тренировочных данных.
\textbf{Скалярное умножение векторов} — это операция, результатом которой является число (скаляр), получаемое путем поэлементного умножения соответствующих компонент векторов и последующей суммы этих произведений. Для двух векторов $\mathbf{a} = [a_1, a_2, \ldots, a_n]$ и $\mathbf{b} = [b_1, b_2, \ldots, b_n]$ скалярное умножение обозначается как $\mathbf{a} \cdot \mathbf{b}$ или $\langle \mathbf{a}, \mathbf{b} \rangle$, и вычисляется следующим образом: $ \mathbf{a} \cdot \mathbf{b} = a_1 \cdot b_1 + a_2 \cdot b_2 + \ldots + a_n \cdot b_n $. Этот процесс представляет собой взвешенную сумму произведений компонент векторов и используется в различных математических и физических контекстах.
Пусть даны два формальных нейрона $F^1=(S^1, X^1, Y^1, g^1)$ и $F^2=(S^2, X^2, Y^2, g^2)$. Пусть $i$ - номер компоненты выходного вектора $\overline{y^1}(t)\in Y^1$, a $j$ - номер компоненты входного вектора $\overline{x^2}(t)\in X^2$, где $t\in N$. Тогда будем говорить, что i-я компонента выхода функционального элемента $F^1$ \textbf{соединена} с j-й компонентой входа функционального элемента $F^2$ , если $\overline{x^2_j}(t):=\overline{y^1_i}(t)$.
В математике и линейной алгебре, \textbf{матрица} представляет собой двумерный массив чисел, символов или выражений, разделенных по строкам и столбцам. Обычно матрицы обозначаются заглавными буквами. Например, матрица $A$ может выглядеть следующим образом: $ A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix} $, где каждый элемент $a_{ij}$ находится в $i$-й строке и $j$-м столбце. Размерность матрицы указывается в виде $m \times n$, где $m$ - количество строк, а $n$ - количество столбцов. В приведенном выше примере, размерность матрицы $A$ - $3 \times 2$. Размерность некоторого множества $X$ — это число, обозначаемое как $dim X$, равное количеству элементов в этом множестве. 
Пусть дано множество $E=\{F^i\}^m_{i=1}$, состоящее из $m$ нейронов, и два нейрона $F^i=(S^i, X^i, Y^i, g^i)$ и $F^j=(S^j, X^j, Y^j, g^j)$, $1<=i$, $j<=m$, связаны между собой каким-либо образом. Пусть также размерности множеств равны $dimX^i=N^i_X$, $dimY^i=N^i_Y$, $dimS^i=N^i_S$ и $dimX^j=N^j_X$, $dimY^j=N^j_Y$, $dimS^j=N^j_S$ соответственно. Определим квадратную матрицу $M_{ij}$ размерности $p_{ij}\times p_{ij}$, где $p_{ij}=max(N^i_x, N^j_y)$. Элементы этой матрицы $M_{ij}^{kl}$ будут равны 1 только для тех $k$, $l$, для которых k-й выход i-го элемента равен l-му входу j-го элемента. В остальных случаях элементы матрицы равны нулю. Данная матрица называется \textbf{матрицей связей нейронов}.
Рассмотрим матрицу $M$, элементами которой являются матрицы $M_{ij}$: $M=||M_{ij}||, i,j=\overline{i,m}$. Такая матрица задаёт связи для всех элементов множества $E$ . Данная матрица называется \textbf{матрицей внутренних связей множества} $E$. Если не все матрицы $M_{ij}$ являются матрицами одного порядка, они дополняются нулями справа и снизу там, где это нужно. Это соответствует добавлению «фиктивных» входов и выходов функциональным элементам множества $E$.
Для всех нейронов множества $E$ рассмотрим общий входной вектор $\overline{x}\in X^1\times X^2\times ...\times X^m $, общий вектор состояния $\overline{s}\in S^1\times S^2\times ...\times S^m$ и общий вектор выходов $\overline{y}\in Y^1\times Y^2\times ...\times Y^m$. \textbf{Искусственная нейронная сеть (нейросеть, ИНС)} – структура, состоящая из связанных между собой нейронов, определяемая следующим образом: $N=(E, G, M, H, \overline{x_0}, \overline{s_0}, T)$, где $E=\{F^i\}^m_{i=1}$- множество формальных нейронов; $G(\overline{s}, \overline{x})=(g^1(\overline{s^1}, \overline{x^1})), g^2(\overline{s^2}, \overline{x^2})), ..., g^m(\overline{s^m}, \overline{x^m})))$ - вектор-функция сети определяющая общий выходной вектор по общему входному вектору и вектору состояния; $M$ – матрица внутренних связей множества $E$; $H:(\overline{s}, \overline{x})\mapsto \overline{s^{'}}$ - некоторая функция, позволяющая изменять вектор состояния сети; $\overline{s_0}, \overline{x_0}$ - начальные значения общего входного вектора и вектора состояния; $T$ - некоторый критерий останова функционирования нейронной сети (в простейшем случае – число тактов).
\textbf{Функционирование нейронной сети} происходит следующим образом: \\1. $v$ в момент времени $t0$ , входной сигнал поступает на входные нейроны сети; \\2.$v$ в момент времени $t1$, сигнал, преобразованный входными нейронами, через связи (аксонов) передается на другие нейроны в соответствии со структурой связей и так далее;\\3. $v$ в некоторый момент $t=k$, сигнал поступает на выходные нейроны сети. \\Результатом работы сети является сигнал, поступивший на выходные аксоны в момент $t=T$ (если критерий останова работы сети задан явно в виде количества тактов функционирования $T$).
Нейроны, часть синапсов которых являются входными, называются \textbf{входными нейронами сети}. //Нейроны, аксон которых является выходным, называются \textbf{выходным нейроном сети}. //Очевидно, что нейронная сеть вычисляет линейные функции, нелинейные функции одного переменного, а также всевозможные суперпозиции – функции от функций, получаемые при каскадном соединении сетей. Таким образом, верно следующее утверждение: с помощью нейронной сети можно задать любую непрерывную функцию, с любой, заданной точностью.
Пусть даны ИНС следующего вида $N^1 = (E^1, G^1, _{11}M, H^1, \overline{x^1_0}, \overline{s^1_0}, T^1)$ и $N^2 = (E^2, G^2, _{22}M, H^2, \overline{x^2_0}, \overline{s^2_0}, T^2)$. Пусть в $E^1$ - $m_1$ элементов $F^i$, $i=\overline{1, m^1}$, а в $E^2$ - $m_2$ элементов $F^j$, $j=\overline{m^1+1, m^1+m^2}$. Матрицы внутренних связей - $_{11}M$ и $_{22}M$ соответственно. Тогда ИНС $N$, являющаяся \textbf{соединением сетей} $N_1$ и $N_2$ , при помощи матриц внешних связей $_{12}M$ и $_{21}M$ определяется как $N = (E, G, M, H, \overline{x_0}, \overline{s_0}, T)$, где: \\множество нейронов $E=E^1\times E^2$ \\начальные значения $\overline{x_0}:=(\overline{x_0^1},\overline{x_0^2})$, $\overline{s_0}:=(\overline{s_0^1},\overline{s_0^2})$ \\матрицу внутренних связей $M$ сети $N$ выглядит следующим образом: $ M:=\left|\left|\begin{matrix} _{11}M_{1,1}&...&_{11}M_{1,m^1}&_{12}M_{1,m^1}&...&_{12}M_{1,m^1+m^2} \\...\\_{11}M_{m^1,1}&...&_{11}M_{m^1,m^1}&_{12}M_{m^1,m^1}&...&_{12}M_{m^1,m^1+m^2}\\_{21}M_{m^1+1,1}&...&_{21}M_{m^1+1,m^1}&_{22}M_{m^1+1,m^1}&...&_{22}M_{m^1+1,m^1+m^2}\\...\\_{21}M_{m^1+m^2,1}&...&_{21}M_{m^1+m^2,m^1}&_{22}M_{m^1+m^2,m^1}&...&_{22}M_{m^1+m^2,m^1+m^2}\end{matrix}\right|\right|$, где $_{21}M$ - матрица, определяющая связь элементов ИНС $N^2$ с элементами $N^1$ , а $_{12}M$ матрица, определяющая связь элементов ИНС $N^1$ с элементами $N^2$\\Операции H и G задаются как $H((\overline{s^1}, \overline{s^2}), (\overline{x^1}, \overline{x^2}))=(H^1(\overline{s^1}, \overline{x^1}), H^2(\overline{s^2}, \overline{x^2}))$ и $G((\overline{s^1}, \overline{s^2}), (\overline{x^1}, \overline{x^2}))=(G^1(\overline{s^1}, \overline{x^1}), G^2(\overline{s^2}, \overline{x^2}))$ \\Количество тактов функционирования может определяться по разному, так как в общем случае $T$ – это критерий останова работы сети. Но в простейшем случае параметр $Т$ может быть определен как $T:=T^1+T^2$ , либо как $T:=max(T^1, T^2)$.
Пусть даны ИНС следующего вида $N^1 = (E^1, G^1, _{11}M, H^1, \overline{x^1_0}, \overline{s^1_0}, T^1)$ и $N^2 = (E^2, G^2, _{22}M, H^2, \overline{x^2_0}, \overline{s^2_0}, T^2)$. Пусть в $E^1$ - $m_1$ элементов $F^i$, $i=\overline{1, m^1}$, а в $E^2$ - $m_2$ элементов $F^j$, $j=\overline{m^1+1, m^1+m^2}$. Матрицы внутренних связей - $_{11}M$ и $_{22}M$ соответственно. Тогда ИНС $N$, являющаяся \textbf{соединением сетей} $N_1$ и $N_2$ , при помощи матриц внешних связей $_{12}M$ и $_{21}M$ определяется как $N = (E, G, M, H, \overline{x_0}, \overline{s_0}, T)$, где: \\множество нейронов $E=E^1\times E^2$ \\начальные значения $\overline{x_0}:=(\overline{x_0^1},\overline{x_0^2})$, $\overline{s_0}:=(\overline{s_0^1},\overline{s_0^2})$ \\матрицу внутренних связей $M$ сети $N$ выглядит следующим образом: $M:=\left|\left|\begin{matrix} _{11}M&_{21}M\\_{21}M&_{22}M \end{matrix}\right|\right| $, где $_{21}M$ - матрица, определяющая связь элементов ИНС $N^2$ с элементами $N^1$ , а $_{12}M$ матрица, определяющая связь элементов ИНС $N^1$ с элементами $N^2$\\Операции H и G задаются как $H((\overline{s^1}, \overline{s^2}), (\overline{x^1}, \overline{x^2}))=(H^1(\overline{s^1}, \overline{x^1}), H^2(\overline{s^2}, \overline{x^2}))$ и $G((\overline{s^1}, \overline{s^2}), (\overline{x^1}, \overline{x^2}))=(G^1(\overline{s^1}, \overline{x^1}), G^2(\overline{s^2}, \overline{x^2}))$ \\Количество тактов функционирования может определяться по разному, так как в общем случае $T$ – это критерий останова работы сети. Но в простейшем случае параметр $Т$ может быть определен как $T:=T^1+T^2$ , либо как $T:=max(T^1, T^2)$.
\textbf{Многослойной ИНС (многослойным персептроном)} называется соединение нескольких простых по структуре сетей. Пусть даны $T$ сетей вида $N^i = (E^i, G^i, _{ii}M, H^i, \overline{x^i_0}, \overline{s^i_0}, 1)$, тогда определим их соединение так, что для любого $1<=i<T$ , $N^i$ связана посредством матрицы $_{ii1}M$ с ИНС $N^{i+1}$ и других внешних связей не существует.  У полученной многослойно сети $N = (E, G, M, H, \overline{x_0}, \overline{s_0}, T)$, матрица $M$ будет выглядеть следующим образом: большинство элементов будут нулевыми, за исключением тех элементов, которые расположены непосредственно над элементами главной диагонали, эти элементы будут равны матрицам внутренних связей между простыми ИНС.
\textbf{Полносвязная ИНС} - нейронная сеть, каждый нейрон которой связан со всеми нейронами этой сети, включая самого себя. Все входные сигналы подаются всем нейронам. Выходными сигналами сети могут быть все или некоторые выходные сигналы нейронов после нескольких тактов функционирования сети. Существенное различие между полносвязной и слоистой сетями возникает тогда, когда число тактов функционирования заранее не ограничено – слоистая сеть так работать не может.
